{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disaster tweets comp: introduction to NLP\n",
    "\n",
    "The aim of this notebook is to classify tweets potentially related to catastrophes into real or misleading tweets. It may also serve as an introduction to Natural Language Processing (NLP), since it covers several data cleaning techniques and classification algorithms in order to conclude which combinations are the most succesfull.The spirit of this notebook is not to obtain a top LB score, but to learn how to deal with text data, decide which techniques are more suited for this project and why others are not recommended from a beginner perspective. \n",
    "\n",
    "Hope you enjoy it and, if you like this work, I'll be glad if you upvote it. Let's begin!\n",
    "\n",
    "\n",
    "**TABLE OF CONTENTS**\n",
    "\n",
    "1. [Bag of words (BOW)](#section1)\n",
    "\n",
    "    * [Load data](#section11)\n",
    "    \n",
    "    * [Brief EDA](#section12)\n",
    "    \n",
    "    * [BOW and logistic prediction regression](#section13)\n",
    "\n",
    "\n",
    "2. [Text data cleaning](#section2)\n",
    "\n",
    "    * [Lowercasing](#section21)\n",
    "    \n",
    "    * [Normalization](#section22)\n",
    "  \n",
    "    * [Stop-word removal](#section23)\n",
    "    \n",
    "    * [Lemmatization](#section24)\n",
    "\n",
    "\n",
    "3. [Term frequency - Inverse document frequency (TFIDF)](#section3)\n",
    "\n",
    "\n",
    "4. [Comparison of classification models](#section4)\n",
    "\n",
    "\n",
    "5. [Word embeddings](#section5)\n",
    "\n",
    "    * [Word embeddings](#section51)\n",
    "    \n",
    "    * [Linear SVC model](#section52)\n",
    "\n",
    "\n",
    "6. [Recurrent neural networks (RNN)](#section6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the required libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pycontractions\r\n",
      "  Downloading pycontractions-2.0.1-py3-none-any.whl (9.6 kB)\r\n",
      "Requirement already satisfied: gensim>=2.0 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (3.8.1)\r\n",
      "Requirement already satisfied: pyemd>=0.4.4 in /opt/conda/lib/python3.6/site-packages (from pycontractions) (0.5.1)\r\n",
      "Collecting language-check>=1.0\r\n",
      "  Downloading language-check-1.1.tar.gz (33 kB)\r\n",
      "Requirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.4.1)\r\n",
      "Requirement already satisfied: six>=1.5.0 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.14.0)\r\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.18.1)\r\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.6/site-packages (from gensim>=2.0->pycontractions) (1.9.0)\r\n",
      "Requirement already satisfied: boto3 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.11.15)\r\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.22.0)\r\n",
      "Requirement already satisfied: boto>=2.32 in /opt/conda/lib/python3.6/site-packages (from smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.49.0)\r\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.9.4)\r\n",
      "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.3.3)\r\n",
      "Requirement already satisfied: botocore<1.15.0,>=1.14.15 in /opt/conda/lib/python3.6/site-packages (from boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.14.15)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2019.11.28)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (1.25.8)\r\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (3.0.4)\r\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8)\r\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (2.8.1)\r\n",
      "Requirement already satisfied: docutils<0.16,>=0.10 in /opt/conda/lib/python3.6/site-packages (from botocore<1.15.0,>=1.14.15->boto3->smart-open>=1.8.1->gensim>=2.0->pycontractions) (0.15.2)\r\n",
      "Building wheels for collected packages: language-check\r\n",
      "  Building wheel for language-check (setup.py) ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\r\n",
      "\u001b[?25h  Created wheel for language-check: filename=language_check-1.1-py3-none-any.whl size=90190895 sha256=ae69fc835ac4e6f6ae2bf5b90be720d2de05cefee1e452ff03f162e5de7de6cf\r\n",
      "  Stored in directory: /root/.cache/pip/wheels/ce/fe/32/3b83a67c4f1182f7f6aa134c1d04cdcd893072bdadb4f5a64c\r\n",
      "Successfully built language-check\r\n",
      "Installing collected packages: language-check, pycontractions\r\n",
      "Successfully installed language-check-1.1 pycontractions-2.0.1\r\n",
      "[======================----------------------------] 44.2% 46.3/104.8MB downloaded"
     ]
    }
   ],
   "source": [
    "# General libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import seaborn as sns\n",
    "\n",
    "# Libraries for data cleaning\n",
    "import unidecode\n",
    "import re\n",
    "import string\n",
    "!pip install pycontractions\n",
    "from pycontractions import Contractions\n",
    "import gensim.downloader as api\n",
    "# Choose model accordingly for contractions function\n",
    "model = api.load(\"glove-twitter-25\")\n",
    "# model = api.load(\"glove-twitter-100\")\n",
    "# model = api.load(\"word2vec-google-news-300\")\n",
    "cont = Contractions(kv_model=model)\n",
    "cont.load_models()\n",
    "import operator\n",
    "\n",
    "# NLP libraries\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "!pip install pyspellchecker\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# ML libraries\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# NN libraries\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/nlp-getting-started/train.csv\n",
      "/kaggle/input/nlp-getting-started/test.csv\n",
      "/kaggle/input/nlp-getting-started/sample_submission.csv\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_clean\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/__results__.html\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/submission_svc_basic.csv\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/__output__.json\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/custom.css\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_lemmatized\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/__notebook__.ipynb\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_normalized\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/submission_logistic_basic.csv\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_lowercased\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_no_stopwords\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/__resultx__.html\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/__results___files/__results___8_3.png\n",
      "/kaggle/input/disaster-tweets-comp-introduction-to-nlp/__results___files/__results___10_1.png\n"
     ]
    }
   ],
   "source": [
    "# List of files (including output files)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Bag of words (BOW) <a id=\"section1\"></a>\n",
    "\n",
    "A simple procedure when working with language data is to compute a **bag of words (BOW)**, that consists on creating one column for each word and keeping track of when they appear in the tweets. This will be our first approach given that it's a common technique in NLP projects, significantly fast and works fine without many data cleaning. \n",
    "\n",
    "Once the  BOW has been created, we will train a **logistic regression** model and evaluate the performance of the predictions.\n",
    "\n",
    "Steps in this section:\n",
    "* 1.1. Load data\n",
    "* 1.2. Brief EDA\n",
    "* 1.3. BOW and logistic regression prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Load data <a id=\"section11\"></a>\n",
    "\n",
    "First of all, let's read the data and take a look on its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "",
    "_uuid": ""
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  7613\n",
      "Test dataset size:  3263\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>132</td>\n",
       "      <td>accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>???? it was an accident http://t.co/Oia5fxi4gM</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>133</td>\n",
       "      <td>accident</td>\n",
       "      <td>New Hanover County, NC</td>\n",
       "      <td>FYI CAD:FYI: ;ACCIDENT PROPERTY DAMAGE;WPD;160...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>134</td>\n",
       "      <td>accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8/6/2015@2:09 PM: TRAFFIC ACCIDENT NO INJURY a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>135</td>\n",
       "      <td>accident</td>\n",
       "      <td>global</td>\n",
       "      <td>Aashiqui Actress Anu Aggarwal On Her Near-Fata...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>136</td>\n",
       "      <td>accident</td>\n",
       "      <td>Alberta | Sask. | Montana</td>\n",
       "      <td>Suffield Alberta Accident https://t.co/bPTmlF4P10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>137</td>\n",
       "      <td>accident</td>\n",
       "      <td>Charlotte</td>\n",
       "      <td>9 Mile backup on I-77 South...accident blockin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>138</td>\n",
       "      <td>accident</td>\n",
       "      <td>Baton Rouge, LA</td>\n",
       "      <td>Has an accident changed your life? We will hel...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>139</td>\n",
       "      <td>accident</td>\n",
       "      <td>Hagerstown, MD</td>\n",
       "      <td>#BREAKING: there was a deadly motorcycle car a...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>141</td>\n",
       "      <td>accident</td>\n",
       "      <td>Gloucestershire , UK</td>\n",
       "      <td>@flowri were you marinading it or was it an ac...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>143</td>\n",
       "      <td>accident</td>\n",
       "      <td>NaN</td>\n",
       "      <td>only had a car for not even a week and got in ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id   keyword                   location  \\\n",
       "90  132  accident                        NaN   \n",
       "91  133  accident     New Hanover County, NC   \n",
       "92  134  accident                        NaN   \n",
       "93  135  accident                     global   \n",
       "94  136  accident  Alberta | Sask. | Montana   \n",
       "95  137  accident                  Charlotte   \n",
       "96  138  accident            Baton Rouge, LA   \n",
       "97  139  accident             Hagerstown, MD   \n",
       "98  141  accident       Gloucestershire , UK   \n",
       "99  143  accident                        NaN   \n",
       "\n",
       "                                                 text  target  \n",
       "90     ???? it was an accident http://t.co/Oia5fxi4gM       0  \n",
       "91  FYI CAD:FYI: ;ACCIDENT PROPERTY DAMAGE;WPD;160...       1  \n",
       "92  8/6/2015@2:09 PM: TRAFFIC ACCIDENT NO INJURY a...       1  \n",
       "93  Aashiqui Actress Anu Aggarwal On Her Near-Fata...       1  \n",
       "94  Suffield Alberta Accident https://t.co/bPTmlF4P10       1  \n",
       "95  9 Mile backup on I-77 South...accident blockin...       1  \n",
       "96  Has an accident changed your life? We will hel...       0  \n",
       "97  #BREAKING: there was a deadly motorcycle car a...       1  \n",
       "98  @flowri were you marinading it or was it an ac...       0  \n",
       "99  only had a car for not even a week and got in ...       1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# test = pd.read_csv(\"test.csv\")\n",
    "train = pd.read_csv(\"sentiment_nlp.csv.csv\")\n",
    "\n",
    "print(\"Train dataset size: \", len(train))\n",
    "# print(\"Test dataset size: \", len(test))\n",
    "\n",
    "# A brief look on the train dataset\n",
    "train[90:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Brief EDA <a id=\"section12\"></a>\n",
    "\n",
    "The dataset contains only 3 relevant features, and most of the useful information is contained in the text column. Hence, let's keep things simple and explore only the surface of this data. A few question to answer:\n",
    "* How many different locations?\n",
    "* How many different keywords?\n",
    "* Is the target class balanced? Number of real tweets vs fake tweets\n",
    "* Has the dataset missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique locations:  4521\n",
      "Number of unique keywords:  221\n",
      "Percentage of real disaster tweets:  42.96597924602653\n",
      "Missing values:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "id             0\n",
       "keyword       87\n",
       "location    3638\n",
       "text           0\n",
       "target      3263\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Target')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Join  train and test datasets to analyze all data in single commands\n",
    "all_data = pd.concat([train,test], axis = 0, sort=False)\n",
    "\n",
    "# How many locations/keywords?\n",
    "print(\"Number of unique locations: \", all_data.location.nunique())\n",
    "print(\"Number of unique keywords: \",all_data.keyword.nunique())\n",
    "\n",
    "# Check data balance; frequency of real disaster tweets (target=1)\n",
    "print(\"Percentage of real disaster tweets: \", all_data.target.sum()/len(train)*100)\n",
    "\n",
    "# Find missing values in a single code line,\n",
    "print(\"Missing values:\")\n",
    "display(all_data.isna().sum())\n",
    "\n",
    "ax = sns.barplot(train['target'].value_counts().index,train['target'].value_counts()/len(train))\n",
    "ax.set_title(\"Real vs fake tweets\")\n",
    "ax.set_ylabel(\"Percentage\")\n",
    "ax.set_xlabel(\"Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**:\n",
    "* There's almost half the number of locations than the number of data rows\n",
    "* Not many different keywords\n",
    "* Target variable is balanced (43% are real tweets)\n",
    "* Some keywords are missing\n",
    "* Almost 30% of the locations are missing\n",
    "\n",
    "The most common keywords and locations may also give us some insight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Top 15 locations')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "f1, axes = plt.subplots(1, 2, figsize=(15,5))\n",
    "f1.subplots_adjust(hspace=0.4, wspace=0.3)\n",
    "ax1 = sns.barplot(y=train['keyword'].value_counts()[:15].index,x=train['keyword'].value_counts()[:15],\n",
    "            orient='h', ax=axes[0])\n",
    "ax1.set_title(\"Top 15 keywords\")\n",
    "ax2 = sns.barplot(y=train['location'].value_counts()[:15].index,x=train['location'].value_counts()[:15],\n",
    "            orient='h', ax=axes[1])\n",
    "ax2.set_title(\"Top 15 locations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there are some duplicates (i.e. USA/United States), and some USA countries are also specified (i.e. Chicago, IL). However, since locations have proven to contain lots of missing values, it's not worth working these duplicates. \n",
    "\n",
    "A more interesting task is to **replace missing values** in locations and keywords by the string \"None\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill missings for train and test datasets\n",
    "all_data.location.fillna(\"None\", inplace=True)\n",
    "all_data.keyword.fillna(\"None\", inplace=True)\n",
    "\n",
    "# Fill the target column for the test rows. This will help us with future calls to model predictions\n",
    "all_data.target.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. BOW and logistic regression prediction <a id=\"section13\"></a>\n",
    "\n",
    "Data has been analysed and there are no missing values in the dataset. We could now proceed to analyze all columns in order to classify tweets into real or misleading disaster tweets, and even extract some additional features (number of words, number of punctuation marks, etc). However, since we are interested in the language treatment of this dataset, **for the sake of simplicity I decided to focus only on the text column** and ignore the other ones. Notice that ignoring keywords, locations and engineered features may prevent our algorithms from obtaining better scores.\n",
    "\n",
    "We are in disposition to **compute the bag of words** (BOW), split the dataset into a train/validation subsets and finally compute a **logistic regression model**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.78817734 0.7635468  0.77011494 0.78981938 0.77668309 0.77339901\n",
      " 0.74712644 0.77668309 0.77504105 0.79310345]  ---  0.7753694581280789\n",
      "accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n"
     ]
    }
   ],
   "source": [
    "# Define the vectorizer counter that will compute the BOW\n",
    "def count_vector(data):\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    emb = count_vectorizer.fit_transform(data)\n",
    "    return emb, count_vectorizer\n",
    "\n",
    "\n",
    "# Define a function for the logistic regression algorithm\n",
    "def logreg_bow(data, valid_fraction):\n",
    "    \n",
    "    # Transform data to list\n",
    "    list_corpus = data[\"text\"].tolist()\n",
    "    list_labels = data[\"target\"].tolist()\n",
    "\n",
    "    # Split train-validation data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(list_corpus, list_labels, \n",
    "                                                          test_size=valid_fraction, random_state=21)\n",
    "\n",
    "    # Generate the bag of words through the count_vectorizer function\n",
    "    X_train_counts, count_vectorizer = count_vector(X_train)\n",
    "    X_valid_counts = count_vectorizer.transform(X_valid)\n",
    "\n",
    "    # Run LogisticRegression model\n",
    "    clf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                             multi_class='multinomial', n_jobs=-1, random_state=21)\n",
    "    clf.fit(X_train_counts, y_train)\n",
    "    y_predicted_counts = clf.predict(X_valid_counts)\n",
    "    \n",
    "    # Cross validation score over 10 folds\n",
    "    scores = cross_val_score(clf, X_train_counts, y_train, cv=10)\n",
    "    print(\"Cross validation over 10 folds: \", scores, \" --- \", sum(scores)/10.)\n",
    "\n",
    "    return y_predicted_counts, y_valid\n",
    "\n",
    "\n",
    "# Define a metrics function named get_metrics to evaluate the model's performance\n",
    "def get_metrics(y_test, y_predicted):  \n",
    "    # true positives / (true positives+false positives)\n",
    "    precision = precision_score(y_test, y_predicted, pos_label=None,\n",
    "                                    average='weighted')             \n",
    "    # true positives / (true positives + false negatives)\n",
    "    recall = recall_score(y_test, y_predicted, pos_label=None,\n",
    "                              average='weighted')\n",
    "    \n",
    "    # harmonic mean of precision and recall\n",
    "    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')\n",
    "    \n",
    "    # true positives + true negatives/ total\n",
    "    accuracy = accuracy_score(y_test, y_predicted)\n",
    "    return accuracy, precision, recall, f1\n",
    "\n",
    "\n",
    "# Final call of the logistic regression model and metrics\n",
    "y_predicted_logistic, y_val = logreg_bow(all_data[:len(train)], 0.2)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_val, y_predicted_logistic)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A validation accuracy of 0.797, quite good for such a simple model. Let's compute the prediction over the test dataset and submit the output file to obtain a LB score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.73753281 0.74671916 0.7191601  0.73718791 0.73718791 0.74244415\n",
      " 0.717477   0.71879106 0.73718791 0.72141919]  ---  0.731510721146716\n"
     ]
    }
   ],
   "source": [
    "def submit_logistic(outfile_name, data, test, valid_frac):\n",
    "    \n",
    "    # Run the LogisticRegression model\n",
    "    y_pred, y_valid = logreg_bow(data, valid_frac)\n",
    "    \n",
    "    # Submit results\n",
    "    submission = pd.DataFrame({\n",
    "            \"id\": test.id, \n",
    "            \"target\": y_pred\n",
    "        })\n",
    "    submission.to_csv(outfile_name, index=False)\n",
    "    \n",
    "    return y_pred, y_valid\n",
    "    \n",
    "y_logistic_pred, y_logistic_test = submit_logistic(\"submission_logistic_basic.csv\", all_data, test, len(test)/len(all_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of the models:\n",
    "* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Text data cleaning <a id=\"section2\"></a>\n",
    "\n",
    "Despite being very simple, our first model has reached a reasonably good accuracy. Let's see if we can improve it by performing specific cleaning techniques for language data. \n",
    "\n",
    "Steps in this section:\n",
    "* 2.1. Lowercasing\n",
    "* 2.2. Normalization\n",
    "* 2.3. Stop word removal\n",
    "* 2.4. Lemmatization\n",
    "\n",
    "**Note 1**: this techniques should not be considered as a mandatory procedure to apply in all NLP projects. Some of them could lead to lower performances depending on the specific nature of the problem, and that's why we are going to add them to the model independently in order to verify if the accuracy increases.\n",
    "\n",
    "**Note 2**: to achieve a more readable code, we will compute everything based on generalized functions that include all the previous steps. Moreover, some of the cleaning techniques take some time to process the data, so that we will save one file per transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Lowercasing <a id=\"section21\"></a>\n",
    "\n",
    "**Definition**: In order to allow the program to generalize words, transform all of them to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert text column to lowercase\n",
    "all_data2 = all_data.copy()\n",
    "all_data2['text'] = all_data['text'].apply(lambda x: x.lower())\n",
    "all_data2.head(5)\n",
    "all_data2.to_csv(\"tweets_lowercased\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tokenizer works fine, transforming all characters into lowercase. Does this transformation improves the model's performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.78817734 0.7635468  0.77011494 0.78981938 0.77668309 0.77339901\n",
      " 0.74712644 0.77668309 0.77504105 0.79310345]  ---  0.7753694581280789\n",
      "accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n"
     ]
    }
   ],
   "source": [
    "y_logistic_lower, y_val = logreg_bow(all_data2[:len(train)], 0.2)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_lower)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: lowercased text do not improve the logistic regression performance. However, it does not make it worse either.\n",
    "\n",
    "Performance of the models:\n",
    "* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Normalization <a id=\"section22\"></a>\n",
    "\n",
    "**Definition**: The jargon used in coloquial language, specially in social media, leads to either content not useful for NLP or to different versions of the same sentence but written in alternative ways. For example: punctuation marks, emojis, contractions, URLs, grammar errors... \n",
    "\n",
    "To normalize data and reduce noise, we will apply several transformations:\n",
    "* **Remove accents**. Instead of receiving accented characters (latt) we just write plain letters.\n",
    "* **Remove URL**. Most tweets may include links and such, it may be good to clean them.\n",
    "* **Remove html**. Scratched data usually contains headers and marks (<br/), this should be removed.\n",
    "* **Remove emojis**. Despite emojis are related to sentiments, they are abused and used in any type of tweet.\n",
    "* **Remove punctuation**. All punctuation marks are deleted.\n",
    "* **Expand contraction**s. Contractions like isn't are expanded to is not.\n",
    "* **Spell checking**. Substitute wrong sentences (Am gona ned you) to correct ones (I am going to need you).\n",
    "\n",
    "**Disclaimer**: the origin of several of the functions used in this subsection come from the awesome kernel of Shahules786; https://www.kaggle.com/shahules/basic-eda-cleaning-and-glove. Please check it out for reckon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accented_chars(text):\n",
    "    text = unidecode.unidecode(text)\n",
    "    return text\n",
    "\n",
    "def remove_html(text):\n",
    "    html=re.compile(r'<.*?>')\n",
    "    return html.sub(r'',text)\n",
    "\n",
    "def remove_url(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url.sub(r'',text)\n",
    "\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "def expand_contractions(text):\n",
    "    text = list(cont.expand_texts([text], precise=True))[0]\n",
    "    return text\n",
    "\n",
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "# Now compact all the normalization function calls into a single function\n",
    "def normalization(text):\n",
    "    text = remove_accented_chars(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_url(text)\n",
    "    text = remove_emoji(text)\n",
    "    text = remove_punct(text)\n",
    "    text = expand_contractions(text)\n",
    "    text = correct_spellings(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all the sub-processes have been defined, we just need to apply the normalization function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  5954.181267023087\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "all_data2 = all_data.copy()\n",
    "all_data2['text'] = all_data['text'].apply(lambda x: normalization(x))\n",
    "all_data2.head(5)\n",
    "all_data2.to_csv(\"tweets_normalized\", index=False)\n",
    "\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets have been correctly transformed according to our normalization rules. Do normalization increases the logistic regression performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.75697865 0.76683087 0.75041051 0.78489327 0.7454844  0.75697865\n",
      " 0.74220033 0.76190476 0.74712644 0.78489327]  ---  0.7597701149425287\n",
      "accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788\n"
     ]
    }
   ],
   "source": [
    "y_logistic_norm, y_val = logreg_bow(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_normalized\")[:len(train)], 0.2)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_norm)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: looks like normalization has worsen the model's accuracy, as well as the rest of the metrics. Maybe some of the removed characters contained some useful information, so that we conclude that no normalization is required for the dataset.\n",
    "\n",
    "Performance of the models:\n",
    "* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Normalized logistic**:  cross_val_accuracy = 0.7599, accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3. Stop word removal <a id=\"section23\"></a>\n",
    "\n",
    "**Definition**: Analogous to normalization, some words are merely argon based and do not add much value to sentences. For example: the, to, and,...\n",
    "\n",
    "In order to remove stop words, we will use a powerful NLP library named spaCy. **SpaCy is able to recognise different languages**, split full texts into words (tokens) and extract additional features that we will review in the following sections. For now on, let's just use it for transformation purposes.\n",
    "\n",
    "The first step to work with spaCy is to load the language model to use (see https://spacy.io/usage/models and https://spacy.io/usage/spacy-101 for detailed info), in our case English. This sets up the specific rules of the language in order to perform the **tokenization**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spacy model to get sentence vectors\n",
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SpaCy library contains an in-built stop word function, so that removing these words is very simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  112.17420625686646\n"
     ]
    }
   ],
   "source": [
    "def remove_stopwords(text):\n",
    "    tokens = [token.text for token in text if not token.is_stop]\n",
    "    return ' '.join([token for token in tokens])\n",
    "\n",
    "ts = time.time()\n",
    "all_data2 = all_data.copy()\n",
    "all_data2['text'] = all_data['text'].apply(lambda x: remove_stopwords(nlp(x)))\n",
    "all_data2.head(5)\n",
    "all_data2.to_csv(\"tweets_no_stopwords\", index=False)\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopwords are succesfully removed from the tweets text. Let's see if this improves the performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.77832512 0.77175698 0.75533662 0.80788177 0.78817734 0.77175698\n",
      " 0.74876847 0.76518883 0.77504105 0.80952381]  ---  0.777175697865353\n",
      "accuracy = 0.787, precision = 0.786, recall = 0.787, f1 = 0.786\n"
     ]
    }
   ],
   "source": [
    "y_logistic_stop, y_val = logreg_bow(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_no_stopwords\")[:len(train)], 0.2)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_stop)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: removing stopwords has slightly increased the cross validation accuracy. Other metrics are also improved, so that we conclude that stop word removal helps the model to predict real disaster tweets. \n",
    "\n",
    "Performance of the models:\n",
    "* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Normalized logistic**:  cross_val_accuracy = 0.7599, accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788\n",
    "* **No stopwords logistic**:   cross_val_accuracy = 0.7755, accuracy = 0.787, precision = 0.786, recall = 0.787, f1 = 0.786"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Lemmatization <a id=\"section24\"></a>\n",
    "\n",
    "**Definition**: Several words share a common root, but their slight differences are not very important. Hence, lemmatization translates all these words into their root form, so that models are not confused by different versions. For example: bad and worse, or flying, flew and flown.\n",
    "\n",
    "Again, we will use Spacy, since it automatically computes the lemma of each word. Analogously to stop words, obtaining the lemma is very easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  102.94515037536621\n"
     ]
    }
   ],
   "source": [
    "def lemmatizer(text):\n",
    "    tokens = [token.lemma_ for token in text]\n",
    "    return ' '.join([token for token in tokens])\n",
    "\n",
    "ts = time.time()\n",
    "all_data2 = all_data.copy()\n",
    "all_data2['text'] = all_data['text'].apply(lambda x: lemmatizer(nlp(x)))\n",
    "all_data2.head(5)\n",
    "all_data2.to_csv(\"tweets_lemmatized\", index=False)\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization has tranformed all words into their root form. Once a model is run, it will recognize the meaning of the words but ignoring specific conjugations and alternative forms. Our next objective is to verify if the accuracy is improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.78981938 0.76190476 0.76190476 0.7816092  0.77339901 0.7865353\n",
      " 0.75205255 0.77504105 0.78325123 0.80131363]  ---  0.7766830870279146\n",
      "accuracy = 0.789, precision = 0.789, recall = 0.789, f1 = 0.789\n"
     ]
    }
   ],
   "source": [
    "y_logistic_lemma, y_val = logreg_bow(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_lemmatized\")[:len(train)], 0.2)\n",
    "accuracy, precision, recall, f1 = get_metrics(y_val, y_logistic_lemma)\n",
    "print(\"accuracy = %.3f, precision = %.3f, recall = %.3f, f1 = %.3f\" % (accuracy, precision, recall, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: lemmatization has proved to slightly increase the tweets classification process. We will apply lemmatization in future models. From all the techniques, normalization is the only one that has provided a lower accuracy. Hence, we conclude that we should avoid normalization for this dataset.\n",
    "\n",
    "Performance of the models:\n",
    "* **Basic logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Lowercase logistic**:  cross_val_accuracy = 0.7754, accuracy = 0.797, precision = 0.797, recall = 0.797, f1 = 0.797\n",
    "* **Normalized logistic**:  cross_val_accuracy = 0.7598, accuracy = 0.788, precision = 0.788, recall = 0.788, f1 = 0.788\n",
    "* **No stopwords logistic**:   cross_val_accuracy = 0.7772, accuracy = 0.787, precision = 0.786, recall = 0.787, f1 = 0.786\n",
    "* **Lemmatized logistic**:   cross_val_accuracy = 0.7767, accuracy = 0.776, precision = 0.776, recall = 0.776, f1 = 0.776"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Term frequency - Inverse document frequency (TFIDF) <a id=\"section3\"></a>\n",
    "\n",
    "Data cleaning has demonstrated its utility, but also that it's important to explore which techniques are suitable for our problem. In order to keep getting better results, we'll try a subtle modification of the BOW named **term frequency, inverse document frequency (TF-IDF)**. \n",
    "\n",
    "Analogous to BOW, TF-IDF creates one column for each word, but this time the weight is averaged by both the frequency of the word and the number of cases in which it appears. Hence, if there are multiple occurrences of a certain word, but it's common in most of the texts, then the weight is lowered. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf(data):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    train = tfidf_vectorizer.fit_transform(data)\n",
    "    return train, tfidf_vectorizer\n",
    "\n",
    "def logreg_tfidf(data, valid_fraction):\n",
    "    \n",
    "    # Transform data to list\n",
    "    list_corpus = data[\"text\"].tolist()\n",
    "    list_labels = data[\"target\"].tolist()\n",
    "    \n",
    "    # Split train-validation data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(list_corpus, list_labels, \n",
    "                                                          test_size=valid_fraction, random_state=21)\n",
    "\n",
    "    # Compute the tfidf vectors\n",
    "    X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "    X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "\n",
    "    # Run the logistic regression model\n",
    "    clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver='newton-cg', \n",
    "                             multi_class='multinomial', n_jobs=-1, random_state=21)\n",
    "    clf_tfidf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "    y_predicted_tfidf = clf_tfidf.predict(X_valid_tfidf)\n",
    "    \n",
    "    # Cross validation score over 10 folds\n",
    "    scores = cross_val_score(clf_tfidf, X_train_tfidf, y_train, cv=10)\n",
    "    print(\"Cross validation over 10 folds: \", sum(scores)/10.)\n",
    "    \n",
    "    return y_predicted_tfidf, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to properly analyze the results of the TF-IDF transformation, we will run the logistic regression model for all the data cleaning cases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic model\n",
      "Cross validation over 10 folds:  0.7773399014778325\n",
      "Lowercased model\n",
      "Cross validation over 10 folds:  0.7773399014778325\n",
      "Normalized model\n",
      "Cross validation over 10 folds:  0.7674876847290639\n",
      "No stopwords model\n",
      "Cross validation over 10 folds:  0.777832512315271\n",
      "Lemmatized model\n",
      "Cross validation over 10 folds:  0.7802955665024631\n"
     ]
    }
   ],
   "source": [
    "print(\"Basic model\")\n",
    "y_predicted_tfidf, y_valid = logreg_tfidf(all_data[:len(train)], 0.2)\n",
    "\n",
    "print(\"Lowercased model\")\n",
    "y_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_lowercased\")[:len(train)], 0.2)\n",
    "\n",
    "print(\"Normalized model\")\n",
    "y_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_normalized\")[:len(train)], 0.2)\n",
    "\n",
    "print(\"No stopwords model\")\n",
    "y_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_no_stopwords\")[:len(train)], 0.2)\n",
    "\n",
    "print(\"Lemmatized model\")\n",
    "y_predicted_tfidf, y_valid = logreg_tfidf(pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_lemmatized\")[:len(train)], 0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion**: results are slightly better than in the BOW case, but they point into the same direction with respect to data cleaning. All  cleaning techniques have improved the model's accuracy, except for normalization. From now on, we will use a dataset with the **full cleaning but for normalization**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Comparison of classification models <a id=\"section4\"></a>\n",
    "\n",
    "At this point, we are in disposition to conclude that the best results are obtained by a combination of TF-IDF and a certain data cleaning (no normalization). At least, that's what we observed from a Logistic regression model. But, what about other models? In this section we will analyze different classification algorithms to see which performs better in the tweets dataset.\n",
    "\n",
    "Models to study:\n",
    "* Naive Bayes\n",
    "* Linear SVC\n",
    "* KNN\n",
    "* Random Forest\n",
    "* XGBoost\n",
    "\n",
    "Before starting with the predictions, we need to generate the clean tweet dataset to analyze (lowercasing + stop removal + lemmatization):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  95.30118370056152\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "# Read the file with no stop words\n",
    "clean_data = pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_no_stopwords\")\n",
    "\n",
    "# Lemmatize\n",
    "clean_data['text'] = clean_data['text'].apply(lambda x: lemmatizer(nlp(x)))\n",
    "\n",
    "# Lowercase\n",
    "clean_data['text'] = clean_data['text'].apply(lambda x: x.lower())\n",
    "\n",
    "clean_data.to_csv(\"tweets_clean\", index=False)\n",
    "\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a couple quality of life functions to split train/test data and get the cross validation score of any classification model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a tfidf and split function\n",
    "def tfidf_split(data, valid_fraction):\n",
    "    \n",
    "    # Transform data to list\n",
    "    list_corpus = data[\"text\"].tolist()\n",
    "    list_labels = data[\"target\"].tolist()\n",
    "    \n",
    "    # Split train-validation data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(list_corpus, list_labels, \n",
    "                                                          test_size=valid_fraction, random_state=21)\n",
    "\n",
    "    # Compute the tfidf vectors\n",
    "    X_train_tfidf, tfidf_vectorizer = tfidf(X_train)\n",
    "    X_valid_tfidf = tfidf_vectorizer.transform(X_valid)\n",
    "    \n",
    "    return X_train_tfidf, X_valid_tfidf, y_train, y_valid\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = tfidf_split(clean_data[:len(train)], 0.2)\n",
    "\n",
    "\n",
    "# Define a general call for the different models\n",
    "def get_cross_val(model, X_train, X_valid, y_train, y_valid):\n",
    "    \n",
    "    # Fit on train, predict on validation\n",
    "    clf = model\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_valid)\n",
    "    \n",
    "    # Cross validation score over 10 folds\n",
    "    scores = cross_val_score(clf, X_train, y_train, cv=10)\n",
    "    print(\"Cross validation over 10 folds: \", sum(scores)/10.)\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to obtain the cross validation score, we simply call the function **get_cross_val** for each of the models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic regression: \n",
      "Cross validation over 10 folds:  0.7763546798029556\n",
      "\n",
      "Naive Bayes:\n",
      "Cross validation over 10 folds:  0.6021346469622332\n",
      "\n",
      "SVC:\n",
      "Cross validation over 10 folds:  0.789655172413793\n",
      "\n",
      "KNN:\n",
      "Cross validation over 10 folds:  0.777175697865353\n",
      "\n",
      "Random forest:\n",
      "Cross validation over 10 folds:  0.7821018062397372\n",
      "\n",
      "XGBoost:\n",
      "Cross validation over 10 folds:  0.7819376026272578\n",
      "Time spent:  1765.5028131008148\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "print(\"Logistic regression: \")\n",
    "y_LR = get_cross_val(LogisticRegression(C=30.0, class_weight='balanced', \n",
    "                                        solver='newton-cg', multi_class='multinomial', \n",
    "                                        n_jobs=-1, random_state=21), \n",
    "                                        X_train.toarray(), X_valid.toarray(), y_train, y_valid)\n",
    "\n",
    "print(\"\\nNaive Bayes:\")\n",
    "y_bayes = get_cross_val(GaussianNB(), X_train.toarray(), X_valid.toarray(), y_train, y_valid)\n",
    "\n",
    "print(\"\\nSVC:\")\n",
    "y_SVC = get_cross_val(LinearSVC(random_state=21, dual=False), X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "print(\"\\nKNN:\")\n",
    "y_KNN = get_cross_val(KNeighborsClassifier(n_neighbors=15), X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "print(\"\\nRandom forest:\")\n",
    "y_RF = get_cross_val(RandomForestClassifier(random_state=21), X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "print(\"\\nXGBoost:\")\n",
    "y_XGB = get_cross_val(XGBClassifier(n_estimators=1000, random_state=21), X_train, X_valid, y_train, y_valid)\n",
    "\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusions**: All models have performed well except for Naive Bayes. The best scores are obtained by SVC, Random Forest and XGB. Notice that we performed no fine tunning of the parameters, which may affect the results (in particular for complex algorithms like XGB)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Word embeddings <a id=\"section5\"></a>\n",
    "\n",
    "Word embeddings (or word vectors) are a **transformation of words into vectors** based on large language models. Two vectors will be similar when their respective words share a common background; for example, queen and king have similar components, while queen and table don't. I strongly recommend [Mat's Kaggle course](https://www.kaggle.com/matleonard/word-vectors) for more details.\n",
    "\n",
    "On the other hand, we will classify these vectors with  a **Support vector machine** (SVM). SVMs are machine learning algorithms that aim to find an hyperplane in a N-dimensional space, where N is the number of features, in order to classifiy data points into classes. SVM are both precise and fast when dealing with high dimensional datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1. Word embeddings <a id=\"section51\"></a>\n",
    "\n",
    "To transform text into vectors we are going to use the spaCy library, that we previously used for the removal of stop-words. Notice that the word vector transformation is at the level of words, but in this project we are dealing with full sentences (tweets). To transform a whole text into a single vector, spaCy just computes the **average of all vectors (words) in a sentence to create the text vector**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time spent:  99.90186953544617\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "clean_data = pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_clean\")\n",
    "vectors = np.array([nlp(tweet.text).vector for idx, tweet in clean_data.iterrows()])\n",
    "vectors.shape\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another detail to consider is that **language texts from a certain topic frequently share a lot of common words**. Hence, the different sentence vectors may be very similar to each other. To fix this, it is useful to compute the mean of all vectors (i.e. the whole dataset vector) and substract it from them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Center shape:  (10876, 300)\n"
     ]
    }
   ],
   "source": [
    "# Center the vectors\n",
    "vec_mean = vectors.mean(axis=0)\n",
    "centered = pd.DataFrame([vec - vec_mean for vec in vectors])\n",
    "print(\"Center shape: \", centered.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2. Linear SVC model <a id=\"section52\"></a>\n",
    "\n",
    "Finally, data is relatively clean and has been transformed into vectors. The only task remaining is to feed these data into the SVM model, train it and finally submit the results from the test dataset.\n",
    "\n",
    "We will use 80% of data for training and the remaining 20% to validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.78489327 0.79967159 0.7635468  0.77996716 0.7865353  0.80295567\n",
      " 0.7635468  0.83087028 0.77996716 0.79474548]  ---  0.7886699507389163\n",
      "Time spent:  10.491590976715088\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "\n",
    "def svc_model(vectors, train):\n",
    "    # Split train-validation data\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(vectors[:len(train)], train.target, \n",
    "                                                          test_size=0.2, random_state=21)\n",
    "\n",
    "    # Create the LinearSVC model\n",
    "    model = LinearSVC(random_state=21, dual=False)\n",
    "    \n",
    "    # Fit the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Cross validation score over 10 folds\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=10)\n",
    "    print(\"Cross validation over 10 folds: \", scores, \" --- \", sum(scores)/10.)\n",
    "    \n",
    "    # Uncomment to see model accuracy\n",
    "    #print(f'Model test accuracy: {model.score(X_valid, y_valid)*100:.3f}%')\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_svc_basic = svc_model(centered, train)\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A reasonably good accuracy (around 79%) spending only 14 seconds for 10 cross validations folds and using cleaned data, not bad at all. Let's submit the prediction over test dataset in order to see how well performs our first model on the leaderboard, before we move to another section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit results\n",
    "\n",
    "y_test = model_svc_basic.predict(centered[-len(test):])\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test.id, \n",
    "    \"target\": y_test\n",
    "})\n",
    "submission.to_csv('submission_svc_basic.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of the models for 10 cv folds:\n",
    "* **Logistic regression (TFIDF)**:  0.7763546798029556\n",
    "* **Naive Bayes (TFIDF)**:  0.6021346469622332\n",
    "* **SVC (TFIDF)**: 0.789655172413793\n",
    "* **KNN (TFIDF)**:  0.777175697865353\n",
    "* **Random forest (TFIDF)**: 0.7821018062397372\n",
    "* **XGBoost (TFIDF)**: 0.7819376026272578\n",
    "* **SVC (word embeddings)**: 0.7886699507389163"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross validation over 10 folds:  [0.80623974 0.80295567 0.7816092  0.78981938 0.79146141 0.80788177\n",
      " 0.77668309 0.82101806 0.80131363 0.7816092 ]  ---  0.7960591133004927\n",
      "Time spent:  117.43390560150146\n"
     ]
    }
   ],
   "source": [
    "ts = time.time()\n",
    "clean_data = all_data\n",
    "vectors = np.array([nlp(tweet.text).vector for idx, tweet in clean_data.iterrows()])\n",
    "vec_mean = vectors.mean(axis=0)\n",
    "centered = pd.DataFrame([vec - vec_mean for vec in vectors])\n",
    "model_svc_basic = svc_model(centered, train)\n",
    "print(\"Time spent: \", time.time() - ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performance of the models for 10 cv folds:\n",
    "* **Logistic regression (TFIDF)**:  0.7763546798029556\n",
    "* **Naive Bayes (TFIDF)**:  0.6021346469622332\n",
    "* **SVC (TFIDF)**: 0.789655172413793\n",
    "* **KNN (TFIDF)**:  0.777175697865353\n",
    "* **Random forest (TFIDF)**: 0.7821018062397372\n",
    "* **XGBoost (TFIDF)**: 0.7819376026272578\n",
    "* **SVC (word embeddings)**: 0.7886699507389163\n",
    "* **SVC (word embeddings, no clean data)**: 0.7960591133004927"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Recurrent neural networks (RNN) <a id=\"section6\"></a>\n",
    "\n",
    "All our previous attempts to classify tweets have been based on different machine learning algorithms, but none of them considered the potential of deep learning. Both section 6 and 7 will cover this approach to analyze if it can obtain a better classification performance.\n",
    "\n",
    "Our first model is a recurrent neural network (RNN), in particular the so called **long-short term memory (LSTM)** model, a type of neural network that permits a bidirectional flow of information through layers. At some level, the idea is to **replicate the idea of a memory**, since previous segments of data may affect the following ones, hence helping the model to recognise patterns in the sentence. You cna find a clear and detailed exaplanation in [Colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/) (maybe a bit outdated). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file with no stop words\n",
    "clean_data = pd.read_csv(\"/kaggle/input/disaster-tweets-comp-introduction-to-nlp/tweets_clean\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example https://www.kaggle.com/reiinakano/basic-nlp-bag-of-words-tf-idf-word2vec-lstm\n",
    "\n",
    "text_lengths = [len(x.split()) for x in (clean_data['text'])]\n",
    "num_words = max(text_lengths)\n",
    "\n",
    "def model_RNN(num_words, embed_dim, lstm_out, clean_data, batch_size, eta, dropout, n_epochs): \n",
    "    \n",
    "    # Use the Keras tokenizer\n",
    "    tokenizer = Tokenizer(num_words=num_words)\n",
    "    tokenizer.fit_on_texts(clean_data['text'].values)\n",
    "\n",
    "    # Pad the data \n",
    "    X = tokenizer.texts_to_sequences(clean_data['text'].values)\n",
    "    X = pad_sequences(X, maxlen=num_words + 1)\n",
    "\n",
    "    # Split data\n",
    "    Y = pd.get_dummies(clean_data['target']).values\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state = 21, stratify=Y)\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Embedding(num_words, embed_dim, input_length = X.shape[1]))\n",
    "    model.add(LSTM(lstm_out, recurrent_dropout=dropout, dropout=dropout))\n",
    "    model.add(Dense(2,activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=Adam(lr=eta), metrics = ['accuracy'])\n",
    "    model_history = model.fit(X_train, Y_train, epochs=n_epochs, batch_size=batch_size, validation_split=0.2, verbose=0)\n",
    "    \n",
    "    return model, model_history\n",
    "\n",
    "# Define RNN parameters\n",
    "embed_dim = 64\n",
    "lstm_out = 64 \n",
    "batch_size = 16\n",
    "eta = 0.001\n",
    "dropout = 0.5\n",
    "n_epochs = 20\n",
    "n_reps = 10\n",
    "\n",
    "#model_RNN_1, model_RNN_1_hist_rep = model_RNN(num_words, embed_dim, lstm_out, clean_data, batch_size, eta, dropout, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repetition  9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "model_RNN_1_hist = [0]*n_epochs\n",
    "\n",
    "for rep in range(n_reps):\n",
    "    print(\"Repetition \", rep)\n",
    "    model_RNN_1, model_RNN_1_hist_rep = model_RNN(num_words, embed_dim, lstm_out, clean_data, batch_size, eta, 0.2, n_epochs)\n",
    "    model_RNN_1_hist = tuple(map(operator.add, model_RNN_1_hist, model_RNN_1_hist_rep.history['val_accuracy']))\n",
    "    \n",
    "model_RNN_1_hist = [x/n_reps for x in list(model_RNN_1_hist)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-32-a3e38492296f>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-32-a3e38492296f>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    plt.xlabel(\"Epochs\")\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "plt.plot(model_RNN_1_hist\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Val_accuracy\")\n",
    "plt.xlim(0,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-dd0db4e99a58>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mXtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_words\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mRNN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "Xtest = tokenizer.texts_to_sequences(clean_data[-len(train):]['text'].values)\n",
    "Xtest = pad_sequences(Xtest, maxlen=num_words + 1)\n",
    "\n",
    "RNN = model.predict(Xtest)\n",
    "\n",
    "y_test = model.predict(Xtest)\n",
    "y_test = np.argmax(y_test,axis = 1)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"id\": test.id, \n",
    "    \"target\": y_test\n",
    "})\n",
    "submission.to_csv('submission_lstm.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
